{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXdA3xvrMunD"
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "\n",
    "\n",
    "### Read the Dataset\n",
    "\n",
    "- Use Pandas to read the 'covertype.csv' file\n",
    "- The dataset contains information on different forest cover types\n",
    "- Look at the columns. Which of them contain meaningful features?\n",
    "\n",
    "\n",
    "\n",
    "### Seperate Features and Labels\n",
    "- Define x as the vectors of meaningful features\n",
    "- Define y as the labels (Cover_Type)\n",
    "\n",
    "\n",
    "\n",
    "### Split the dataset into two disjoint datasets for training and testing\n",
    "- Randomly split the dataset. Use 70% for training and 30% for testing.\n",
    "- Define x_train and x_test as the feature vectors\n",
    "- Define y_train and y_test as the labels\n",
    "    - Hint: Have a look at the sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XBWsmnGxeJQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"covertype.csv\") #please enter the path where you covertype.csv file is stored.\n",
    "\n",
    "x = df.loc[:, 'Elevation':]\n",
    "x = x.loc[:, x.columns != 'Cover_Type']\n",
    "  \n",
    "y = df['Cover_Type']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state = 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-ALJz-yMunL"
   },
   "source": [
    "### Train a simple deep neural network\n",
    "- Use Keras to define a simple Multi-Layer Perceptron with at least 3 layers and a Softmax classifier\n",
    "    - You have to explicitly give the input shape of the first layer\n",
    "    - The other layer shapes are inferred\n",
    "    - The last layer should have as many neurons as there are classes\n",
    "        - How many classes are there?\n",
    "- Define 'accuracy' as performance metric when compiling the network model\n",
    "- Train the MLP with x_train, y_train\n",
    "    - Make sure to save the training history for later assessment\n",
    "- Evaluate the performance on x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "1Fn-suPdMunM",
    "outputId": "408f77ca-8d96-4841-849b-45fa54eaf2bd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10584/10584 [==============================] - 2s 151us/step - loss: nan - acc: 4.7241e-04\n",
      "Epoch 2/20\n",
      "10584/10584 [==============================] - 1s 122us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "10584/10584 [==============================] - 1s 122us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "10584/10584 [==============================] - 1s 126us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "10584/10584 [==============================] - 1s 127us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "10584/10584 [==============================] - 1s 126us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "10584/10584 [==============================] - 1s 126us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "10584/10584 [==============================] - 1s 126us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "10584/10584 [==============================] - 1s 122us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "10584/10584 [==============================] - 1s 128us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "10584/10584 [==============================] - 1s 127us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "10584/10584 [==============================] - 1s 126us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "10584/10584 [==============================] - 1s 124us/step - loss: nan - acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "10584/10584 [==============================] - 1s 125us/step - loss: nan - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import keras as kr\n",
    "import tensorflow as tf\n",
    "\n",
    "number_of_classes = len(set(y))\n",
    "\n",
    "model = kr.Sequential([\n",
    "    kr.layers.Dense(54, input_dim=x_train.columns.size,activation=tf.nn.softmax),\n",
    "    kr.layers.Dense(128, activation=tf.nn.softmax),\n",
    "    kr.layers.Dense(number_of_classes, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evtqyTTCzg4-"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.plot(hist.epoch, np.array(hist.history['acc']), label='Train Accuracy')\n",
    "    plt.plot(hist.epoch, np.array(hist.history['val_acc']), label = 'Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkTk6be6MunP"
   },
   "source": [
    "### Debug\n",
    "- If your loss is NaN, either your network architecture or your data is faulty\n",
    "    - Check your network architecture\n",
    "    - Check your data\n",
    "        - Are there any NaN or infinite features or labels?\n",
    "    - Print the labels.\n",
    "        - How many unique labels do you have?\n",
    "        - Are they [0, ..., n-1]?\n",
    "            - If not, align them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "sIf_dbiGMunQ",
    "outputId": "9f2f1f68-648e-436a-db3f-e1e4e2e101c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------Are here any NaNs?--------------:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------Are here any inf values?--------------:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------Labels--------------:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('\\n-----------------Are here any NaNs?--------------:\\n')\n",
    "display(df.isnull().values.any())\n",
    "\n",
    "print('\\n-----------------Are here any inf values?--------------:\\n')\n",
    "df.replace([np.inf, -np.inf], np.nan)\n",
    "display(df.isnull().values.any())\n",
    "\n",
    "\n",
    "print('\\n-----------------Labels--------------:\\n')\n",
    "y = df[\"Cover_Type\"]-1\n",
    "labels = set(y)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cql_SFyvMunS"
   },
   "source": [
    "### Train again\n",
    "- Reinitialize or redefine your MLP from above and train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1870
    },
    "colab_type": "code",
    "id": "6LDd3A8wMunT",
    "outputId": "d35ec6da-fa36-4019-c5a2-2ec4ebc6bdde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10584 samples, validate on 4536 samples\n",
      "Epoch 1/54\n",
      "10584/10584 [==============================] - 1s 51us/step - loss: 1.9464 - acc: 0.1439 - val_loss: 1.9442 - val_acc: 0.1404\n",
      "Epoch 2/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.9423 - acc: 0.1440 - val_loss: 1.9404 - val_acc: 0.1404\n",
      "Epoch 3/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.9377 - acc: 0.1808 - val_loss: 1.9360 - val_acc: 0.2216\n",
      "Epoch 4/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.9309 - acc: 0.2511 - val_loss: 1.9266 - val_acc: 0.2582\n",
      "Epoch 5/54\n",
      "10584/10584 [==============================] - 0s 24us/step - loss: 1.9200 - acc: 0.2543 - val_loss: 1.9137 - val_acc: 0.2776\n",
      "Epoch 6/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.9024 - acc: 0.2792 - val_loss: 1.8923 - val_acc: 0.2826\n",
      "Epoch 7/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.8723 - acc: 0.2935 - val_loss: 1.8620 - val_acc: 0.2762\n",
      "Epoch 8/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.8309 - acc: 0.3042 - val_loss: 1.8080 - val_acc: 0.2930\n",
      "Epoch 9/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.7777 - acc: 0.3075 - val_loss: 1.7500 - val_acc: 0.2992\n",
      "Epoch 10/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.7406 - acc: 0.2999 - val_loss: 1.7197 - val_acc: 0.3128\n",
      "Epoch 11/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.6884 - acc: 0.2993 - val_loss: 1.6560 - val_acc: 0.2842\n",
      "Epoch 12/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.6690 - acc: 0.2779 - val_loss: 1.6624 - val_acc: 0.2793\n",
      "Epoch 13/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.6326 - acc: 0.2842 - val_loss: 1.5935 - val_acc: 0.2820\n",
      "Epoch 14/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.6016 - acc: 0.2853 - val_loss: 1.5849 - val_acc: 0.2851\n",
      "Epoch 15/54\n",
      "10584/10584 [==============================] - 0s 27us/step - loss: 1.5709 - acc: 0.2914 - val_loss: 1.6215 - val_acc: 0.2837\n",
      "Epoch 16/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.5567 - acc: 0.2916 - val_loss: 1.5310 - val_acc: 0.2855\n",
      "Epoch 17/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.5518 - acc: 0.2921 - val_loss: 1.5313 - val_acc: 0.2884\n",
      "Epoch 18/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.5231 - acc: 0.2929 - val_loss: 1.5066 - val_acc: 0.2868\n",
      "Epoch 19/54\n",
      "10584/10584 [==============================] - 0s 27us/step - loss: 1.5140 - acc: 0.2940 - val_loss: 1.5042 - val_acc: 0.2884\n",
      "Epoch 20/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.5091 - acc: 0.2903 - val_loss: 1.4912 - val_acc: 0.2873\n",
      "Epoch 21/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4901 - acc: 0.2932 - val_loss: 1.6432 - val_acc: 0.2751\n",
      "Epoch 22/54\n",
      "10584/10584 [==============================] - 0s 27us/step - loss: 1.4984 - acc: 0.2943 - val_loss: 1.4705 - val_acc: 0.2888\n",
      "Epoch 23/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4732 - acc: 0.2943 - val_loss: 1.4636 - val_acc: 0.2868\n",
      "Epoch 24/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.5058 - acc: 0.2894 - val_loss: 1.4569 - val_acc: 0.2866\n",
      "Epoch 25/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4803 - acc: 0.2935 - val_loss: 1.4744 - val_acc: 0.2879\n",
      "Epoch 26/54\n",
      "10584/10584 [==============================] - 0s 27us/step - loss: 1.4745 - acc: 0.2878 - val_loss: 1.4496 - val_acc: 0.2866\n",
      "Epoch 27/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4640 - acc: 0.2947 - val_loss: 1.4510 - val_acc: 0.2881\n",
      "Epoch 28/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4592 - acc: 0.2923 - val_loss: 1.4514 - val_acc: 0.2959\n",
      "Epoch 29/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4935 - acc: 0.2935 - val_loss: 1.4617 - val_acc: 0.2884\n",
      "Epoch 30/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4805 - acc: 0.2948 - val_loss: 1.4382 - val_acc: 0.2895\n",
      "Epoch 31/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4532 - acc: 0.2942 - val_loss: 1.4324 - val_acc: 0.2919\n",
      "Epoch 32/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4500 - acc: 0.2932 - val_loss: 1.4385 - val_acc: 0.2954\n",
      "Epoch 33/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4640 - acc: 0.2893 - val_loss: 1.4533 - val_acc: 0.2870\n",
      "Epoch 34/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4417 - acc: 0.2938 - val_loss: 1.4388 - val_acc: 0.2974\n",
      "Epoch 35/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4492 - acc: 0.2945 - val_loss: 1.4617 - val_acc: 0.2912\n",
      "Epoch 36/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4522 - acc: 0.2958 - val_loss: 1.4367 - val_acc: 0.2921\n",
      "Epoch 37/54\n",
      "10584/10584 [==============================] - 0s 27us/step - loss: 1.4620 - acc: 0.2943 - val_loss: 1.5146 - val_acc: 0.2884\n",
      "Epoch 38/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4603 - acc: 0.2969 - val_loss: 1.4290 - val_acc: 0.2912\n",
      "Epoch 39/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4623 - acc: 0.2948 - val_loss: 1.4560 - val_acc: 0.2884\n",
      "Epoch 40/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4488 - acc: 0.2915 - val_loss: 1.4492 - val_acc: 0.2884\n",
      "Epoch 41/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4623 - acc: 0.2912 - val_loss: 1.4347 - val_acc: 0.2972\n",
      "Epoch 42/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4389 - acc: 0.2918 - val_loss: 1.4361 - val_acc: 0.2895\n",
      "Epoch 43/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4511 - acc: 0.2931 - val_loss: 1.4289 - val_acc: 0.2959\n",
      "Epoch 44/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4258 - acc: 0.2955 - val_loss: 1.4217 - val_acc: 0.2881\n",
      "Epoch 45/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4248 - acc: 0.2937 - val_loss: 1.4566 - val_acc: 0.2908\n",
      "Epoch 46/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4670 - acc: 0.2923 - val_loss: 1.4517 - val_acc: 0.2895\n",
      "Epoch 47/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4294 - acc: 0.2934 - val_loss: 1.4564 - val_acc: 0.2948\n",
      "Epoch 48/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4484 - acc: 0.2978 - val_loss: 1.4379 - val_acc: 0.2978\n",
      "Epoch 49/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4332 - acc: 0.2965 - val_loss: 1.4252 - val_acc: 0.2901\n",
      "Epoch 50/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.4639 - acc: 0.2935 - val_loss: 1.4626 - val_acc: 0.2892\n",
      "Epoch 51/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4269 - acc: 0.2920 - val_loss: 1.4107 - val_acc: 0.2941\n",
      "Epoch 52/54\n",
      "10584/10584 [==============================] - 0s 26us/step - loss: 1.5100 - acc: 0.2839 - val_loss: 1.4697 - val_acc: 0.2917\n",
      "Epoch 53/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.5093 - acc: 0.2943 - val_loss: 1.4778 - val_acc: 0.2985\n",
      "Epoch 54/54\n",
      "10584/10584 [==============================] - 0s 25us/step - loss: 1.4716 - acc: 0.2964 - val_loss: 1.4428 - val_acc: 0.2877\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state = 55)\n",
    "number_of_classes = len(set(y))\n",
    "\n",
    "model = kr.Sequential([\n",
    "    kr.layers.Dense(54, input_shape = (54, ), activation = tf.nn.softmax),\n",
    "    kr.layers.Dense(30, activation = tf.nn.softmax),\n",
    "    kr.layers.Dense(number_of_classes, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(), \n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history1 = model.fit(x_train, y_train, epochs = 54, batch_size = 196, validation_data = (x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "mfm5oSelzQXc",
    "outputId": "a0ea9c6f-ecba-4503-8645-7b9485b924b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4536/4536 [==============================] - 0s 66us/step\n",
      "Test accuracy: 0.28769841275097413\n",
      "Test loss: 1.4428468705064617\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0975c23e80>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "plot_hist(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzjS6DsZMunV"
   },
   "source": [
    "### Does it work?\n",
    "- The loss should now be a number.\n",
    "- Does the network converge?\n",
    "\n",
    "\n",
    "\n",
    "### Inspect the data\n",
    "- Compute the min, max, mean and standard deviation of each feature\n",
    "- What data type do the columns have?\n",
    "- Use Pandas to print the statistics in a table\n",
    "- What could be problematic with the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1723
    },
    "colab_type": "code",
    "id": "8nfMYideMunW",
    "outputId": "a59b2c4f-3a61-4b4a-b65c-301385c4677e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>int64</td>\n",
       "      <td>1863</td>\n",
       "      <td>3849</td>\n",
       "      <td>2749.32</td>\n",
       "      <td>417.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>156.68</td>\n",
       "      <td>110.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>16.50</td>\n",
       "      <td>8.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1343</td>\n",
       "      <td>227.20</td>\n",
       "      <td>210.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>int64</td>\n",
       "      <td>-146</td>\n",
       "      <td>554</td>\n",
       "      <td>51.08</td>\n",
       "      <td>61.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6890</td>\n",
       "      <td>1714.02</td>\n",
       "      <td>1325.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>212.70</td>\n",
       "      <td>30.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>int64</td>\n",
       "      <td>99</td>\n",
       "      <td>254</td>\n",
       "      <td>218.97</td>\n",
       "      <td>22.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>135.09</td>\n",
       "      <td>45.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6993</td>\n",
       "      <td>1511.15</td>\n",
       "      <td>1099.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type1</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type2</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type3</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type4</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type5</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type6</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type7</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type8</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type9</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type10</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type11</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type12</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type13</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type14</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type15</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type16</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type17</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type18</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type19</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type20</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type21</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type22</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type23</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type24</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type25</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type26</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type27</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type28</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type29</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type30</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type31</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type32</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type33</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type34</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type35</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type36</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type37</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type38</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type39</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type40</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Type   Min   Max    Mean     Std\n",
       "Elevation                           int64  1863  3849 2749.32  417.68\n",
       "Aspect                              int64     0   360  156.68  110.09\n",
       "Slope                               int64     0    52   16.50    8.45\n",
       "Horizontal_Distance_To_Hydrology    int64     0  1343  227.20  210.08\n",
       "Vertical_Distance_To_Hydrology      int64  -146   554   51.08   61.24\n",
       "Horizontal_Distance_To_Roadways     int64     0  6890 1714.02 1325.07\n",
       "Hillshade_9am                       int64     0   254  212.70   30.56\n",
       "Hillshade_Noon                      int64    99   254  218.97   22.80\n",
       "Hillshade_3pm                       int64     0   248  135.09   45.90\n",
       "Horizontal_Distance_To_Fire_Points  int64     0  6993 1511.15 1099.94\n",
       "Wilderness_Area1                    int64     0     1    0.24    0.43\n",
       "Wilderness_Area2                    int64     0     1    0.03    0.18\n",
       "Wilderness_Area3                    int64     0     1    0.42    0.49\n",
       "Wilderness_Area4                    int64     0     1    0.31    0.46\n",
       "Soil_Type1                          int64     0     1    0.02    0.15\n",
       "Soil_Type2                          int64     0     1    0.04    0.20\n",
       "Soil_Type3                          int64     0     1    0.06    0.24\n",
       "Soil_Type4                          int64     0     1    0.06    0.23\n",
       "Soil_Type5                          int64     0     1    0.01    0.10\n",
       "Soil_Type6                          int64     0     1    0.04    0.20\n",
       "Soil_Type7                          int64     0     0    0.00    0.00\n",
       "Soil_Type8                          int64     0     1    0.00    0.01\n",
       "Soil_Type9                          int64     0     1    0.00    0.03\n",
       "Soil_Type10                         int64     0     1    0.14    0.35\n",
       "Soil_Type11                         int64     0     1    0.03    0.16\n",
       "Soil_Type12                         int64     0     1    0.02    0.12\n",
       "Soil_Type13                         int64     0     1    0.03    0.17\n",
       "Soil_Type14                         int64     0     1    0.01    0.11\n",
       "Soil_Type15                         int64     0     0    0.00    0.00\n",
       "Soil_Type16                         int64     0     1    0.01    0.09\n",
       "Soil_Type17                         int64     0     1    0.04    0.20\n",
       "Soil_Type18                         int64     0     1    0.00    0.06\n",
       "Soil_Type19                         int64     0     1    0.00    0.06\n",
       "Soil_Type20                         int64     0     1    0.01    0.10\n",
       "Soil_Type21                         int64     0     1    0.00    0.03\n",
       "Soil_Type22                         int64     0     1    0.02    0.15\n",
       "Soil_Type23                         int64     0     1    0.05    0.22\n",
       "Soil_Type24                         int64     0     1    0.02    0.13\n",
       "Soil_Type25                         int64     0     1    0.00    0.01\n",
       "Soil_Type26                         int64     0     1    0.00    0.06\n",
       "Soil_Type27                         int64     0     1    0.00    0.03\n",
       "Soil_Type28                         int64     0     1    0.00    0.02\n",
       "Soil_Type29                         int64     0     1    0.09    0.28\n",
       "Soil_Type30                         int64     0     1    0.05    0.21\n",
       "Soil_Type31                         int64     0     1    0.02    0.15\n",
       "Soil_Type32                         int64     0     1    0.05    0.21\n",
       "Soil_Type33                         int64     0     1    0.04    0.20\n",
       "Soil_Type34                         int64     0     1    0.00    0.04\n",
       "Soil_Type35                         int64     0     1    0.01    0.08\n",
       "Soil_Type36                         int64     0     1    0.00    0.03\n",
       "Soil_Type37                         int64     0     1    0.00    0.05\n",
       "Soil_Type38                         int64     0     1    0.05    0.21\n",
       "Soil_Type39                         int64     0     1    0.04    0.20\n",
       "Soil_Type40                         int64     0     1    0.03    0.17"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "stats = pd.DataFrame(columns=[\"Type\", \"Min\", \"Max\", \"Mean\", \"Std\"])\n",
    "\n",
    "for col in x.columns:\n",
    "    stats.loc[col] = {\"Type\": x[col].dtype,\n",
    "                      \"Min\": x[col].min(),\n",
    "                      \"Max\": x[col].max(),\n",
    "                      \"Mean\": x[col].mean(),\n",
    "                      \"Std\": x[col].std()\n",
    "                     }\n",
    "\n",
    "\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSedbGWtMunY"
   },
   "source": [
    "### Preprocess the Data\n",
    "- Normalize or standardize your data, so all features are at the same scale.\n",
    "    - This will help your network to use all available features and not be biased by some features with large values\n",
    "    - Does it make sense to normalize all columns, or only some?\n",
    "- Hint: Again, look if you find something useful in sklearn\n",
    "\n",
    "\n",
    "- Never use test data to optimize your training! This includes the preprocessing\n",
    "    - Find preprocessing parameters on your training data only!\n",
    "    - Transform all your data with the computed parameters\n",
    "    - You have to remember which of your samples are used for training and which are for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "qgqy9AVlMunZ",
    "outputId": "704746cb-2f76-46d1-c83c-92846abe3174",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   1      -0.37   -0.96  -1.60                              0.15   \n",
       "1   2      -0.38   -0.91  -1.72                             -0.07   \n",
       "2   3       0.13   -0.16  -0.89                              0.19   \n",
       "3   4       0.09   -0.02   0.18                              0.07   \n",
       "4   5      -0.37   -1.01  -1.72                             -0.35   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                           -0.83                            -0.91   \n",
       "1                           -0.93                            -1.00   \n",
       "2                            0.23                             1.11   \n",
       "3                            1.09                             1.04   \n",
       "4                           -0.85                            -1.00   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm     ...      Soil_Type32  \\\n",
       "0           0.27            0.57           0.28     ...                0   \n",
       "1           0.24            0.70           0.35     ...                0   \n",
       "2           0.70            0.83          -0.00     ...                0   \n",
       "3           0.83            0.83          -0.29     ...                0   \n",
       "4           0.24            0.66           0.32     ...                0   \n",
       "\n",
       "   Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type  \n",
       "0            0            0            0           5  \n",
       "1            0            0            0           5  \n",
       "2            0            0            0           2  \n",
       "3            0            0            0           2  \n",
       "4            0            0            0           5  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler as sc\n",
    "df_new = df\n",
    "df_to_norm = df_new[['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways','Hillshade_9am',\n",
    "   'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']]\n",
    "\n",
    "scaler = sc().fit(df_to_norm.values)\n",
    "scaled_array = scaler.transform(df_to_norm.values)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_array, index=df_to_norm.index, columns=df_to_norm.columns)\n",
    "df_new.update(scaled_df)\n",
    "df_new.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIxF24DM2KtN"
   },
   "outputs": [],
   "source": [
    "x_new = df_new.drop(['Cover_Type', 'Id'], axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.3, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKy59hToMunb"
   },
   "source": [
    "### Inspect data again\n",
    "- Print the statistics of the preprocessed data using the code from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "eVVqPzokMunc",
    "outputId": "df33d85c-f736-457d-8619-546d6b54e2da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>float64</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>4.20</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>float64</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>float64</td>\n",
       "      <td>-6.96</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>float64</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>float64</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>float64</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Type   Min  Max  Mean  Std\n",
       "Elevation                           float64 -2.12 2.63  0.00 1.00\n",
       "Aspect                              float64 -1.42 1.85  0.00 1.00\n",
       "Slope                               float64 -1.95 4.20 -0.00 1.00\n",
       "Horizontal_Distance_To_Hydrology    float64 -1.08 5.31  0.00 1.00\n",
       "Vertical_Distance_To_Hydrology      float64 -3.22 8.21 -0.00 1.00\n",
       "Horizontal_Distance_To_Roadways     float64 -1.29 3.91  0.00 1.00\n",
       "Hillshade_9am                       float64 -6.96 1.35 -0.00 1.00\n",
       "Hillshade_Noon                      float64 -5.26 1.54  0.00 1.00\n",
       "Hillshade_3pm                       float64 -2.94 2.46  0.00 1.00\n",
       "Horizontal_Distance_To_Fire_Points  float64 -1.37 4.98  0.00 1.00"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "stats = pd.DataFrame(columns=[\"Type\", \"Min\", \"Max\", \"Mean\", \"Std\"])\n",
    "\n",
    "\n",
    "for col in df_to_norm.columns.values:\n",
    "    stats.loc[col] = {\"Type\": df_new[col].dtype,\n",
    "                      \"Min\": df_new[col].min(),\n",
    "                      \"Max\": df_new[col].max(),\n",
    "                      \"Mean\": df_new[col].mean(),\n",
    "                      \"Std\": df_new[col].std()\n",
    "                     }\n",
    "\n",
    "\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8WUtYgfMunf"
   },
   "source": [
    "### Train the network again\n",
    "- Reinitialize or redefine your MLP from above and train it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6715
    },
    "colab_type": "code",
    "id": "gdwY-642Munh",
    "outputId": "83b7b42c-1e8f-4b7b-fc55-34ead50a31fd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10584 samples, validate on 4536 samples\n",
      "Epoch 1/196\n",
      "10584/10584 [==============================] - 2s 152us/step - loss: 1.6253 - acc: 0.3221 - val_loss: 1.0778 - val_acc: 0.5895\n",
      "Epoch 2/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 1.1670 - acc: 0.5047 - val_loss: 0.8557 - val_acc: 0.6609\n",
      "Epoch 3/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 1.0099 - acc: 0.5682 - val_loss: 0.7898 - val_acc: 0.6726\n",
      "Epoch 4/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.9543 - acc: 0.5942 - val_loss: 0.7595 - val_acc: 0.6817\n",
      "Epoch 5/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.9014 - acc: 0.6225 - val_loss: 0.7293 - val_acc: 0.6975\n",
      "Epoch 6/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.8692 - acc: 0.6370 - val_loss: 0.7183 - val_acc: 0.6931\n",
      "Epoch 7/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.8362 - acc: 0.6593 - val_loss: 0.6951 - val_acc: 0.7006\n",
      "Epoch 8/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.8319 - acc: 0.6547 - val_loss: 0.6821 - val_acc: 0.7121\n",
      "Epoch 9/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.7911 - acc: 0.6651 - val_loss: 0.6718 - val_acc: 0.7099\n",
      "Epoch 10/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.7824 - acc: 0.6758 - val_loss: 0.6602 - val_acc: 0.7202\n",
      "Epoch 11/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.7683 - acc: 0.6805 - val_loss: 0.6508 - val_acc: 0.7233\n",
      "Epoch 12/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.7676 - acc: 0.6829 - val_loss: 0.6410 - val_acc: 0.7304\n",
      "Epoch 13/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.7390 - acc: 0.6982 - val_loss: 0.6354 - val_acc: 0.7403\n",
      "Epoch 14/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.7332 - acc: 0.6955 - val_loss: 0.6322 - val_acc: 0.7381\n",
      "Epoch 15/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.7267 - acc: 0.7022 - val_loss: 0.6205 - val_acc: 0.7440\n",
      "Epoch 16/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.7310 - acc: 0.7007 - val_loss: 0.6169 - val_acc: 0.7476\n",
      "Epoch 17/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.7119 - acc: 0.7046 - val_loss: 0.6123 - val_acc: 0.7493\n",
      "Epoch 18/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.7015 - acc: 0.7117 - val_loss: 0.6056 - val_acc: 0.7482\n",
      "Epoch 19/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6970 - acc: 0.7121 - val_loss: 0.6045 - val_acc: 0.7469\n",
      "Epoch 20/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.7057 - acc: 0.7145 - val_loss: 0.6005 - val_acc: 0.7533\n",
      "Epoch 21/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6915 - acc: 0.7187 - val_loss: 0.5970 - val_acc: 0.7502\n",
      "Epoch 22/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.6795 - acc: 0.7203 - val_loss: 0.5901 - val_acc: 0.7584\n",
      "Epoch 23/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6845 - acc: 0.7217 - val_loss: 0.5891 - val_acc: 0.7544\n",
      "Epoch 24/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6739 - acc: 0.7237 - val_loss: 0.5938 - val_acc: 0.7498\n",
      "Epoch 25/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6659 - acc: 0.7271 - val_loss: 0.5779 - val_acc: 0.7650\n",
      "Epoch 26/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6630 - acc: 0.7272 - val_loss: 0.5829 - val_acc: 0.7610\n",
      "Epoch 27/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.6625 - acc: 0.7291 - val_loss: 0.5718 - val_acc: 0.7626\n",
      "Epoch 28/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6545 - acc: 0.7370 - val_loss: 0.5709 - val_acc: 0.7590\n",
      "Epoch 29/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6582 - acc: 0.7364 - val_loss: 0.5714 - val_acc: 0.7617\n",
      "Epoch 30/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6476 - acc: 0.7332 - val_loss: 0.5756 - val_acc: 0.7628\n",
      "Epoch 31/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6447 - acc: 0.7370 - val_loss: 0.5643 - val_acc: 0.7668\n",
      "Epoch 32/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6433 - acc: 0.7437 - val_loss: 0.5595 - val_acc: 0.7654\n",
      "Epoch 33/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6390 - acc: 0.7418 - val_loss: 0.5567 - val_acc: 0.7681\n",
      "Epoch 34/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6221 - acc: 0.7489 - val_loss: 0.5557 - val_acc: 0.7716\n",
      "Epoch 35/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6343 - acc: 0.7445 - val_loss: 0.5578 - val_acc: 0.7679\n",
      "Epoch 36/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6280 - acc: 0.7523 - val_loss: 0.5611 - val_acc: 0.7676\n",
      "Epoch 37/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6291 - acc: 0.7446 - val_loss: 0.5528 - val_acc: 0.7751\n",
      "Epoch 38/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6180 - acc: 0.7488 - val_loss: 0.5484 - val_acc: 0.7743\n",
      "Epoch 39/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6213 - acc: 0.7543 - val_loss: 0.5531 - val_acc: 0.7731\n",
      "Epoch 40/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6156 - acc: 0.7528 - val_loss: 0.5484 - val_acc: 0.7765\n",
      "Epoch 41/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6092 - acc: 0.7526 - val_loss: 0.5461 - val_acc: 0.7736\n",
      "Epoch 42/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6196 - acc: 0.7460 - val_loss: 0.5446 - val_acc: 0.7745\n",
      "Epoch 43/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5988 - acc: 0.7558 - val_loss: 0.5454 - val_acc: 0.7738\n",
      "Epoch 44/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.6074 - acc: 0.7580 - val_loss: 0.5355 - val_acc: 0.7749\n",
      "Epoch 45/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.6070 - acc: 0.7580 - val_loss: 0.5373 - val_acc: 0.7751\n",
      "Epoch 46/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.6045 - acc: 0.7524 - val_loss: 0.5347 - val_acc: 0.7760\n",
      "Epoch 47/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5936 - acc: 0.7628 - val_loss: 0.5361 - val_acc: 0.7738\n",
      "Epoch 48/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6045 - acc: 0.7560 - val_loss: 0.5331 - val_acc: 0.7826\n",
      "Epoch 49/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.6052 - acc: 0.7535 - val_loss: 0.5347 - val_acc: 0.7747\n",
      "Epoch 50/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5941 - acc: 0.7621 - val_loss: 0.5350 - val_acc: 0.7769\n",
      "Epoch 51/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5893 - acc: 0.7632 - val_loss: 0.5264 - val_acc: 0.7866\n",
      "Epoch 52/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5848 - acc: 0.7657 - val_loss: 0.5300 - val_acc: 0.7859\n",
      "Epoch 53/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5994 - acc: 0.7633 - val_loss: 0.5260 - val_acc: 0.7890\n",
      "Epoch 54/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5934 - acc: 0.7603 - val_loss: 0.5256 - val_acc: 0.7848\n",
      "Epoch 55/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5894 - acc: 0.7638 - val_loss: 0.5236 - val_acc: 0.7879\n",
      "Epoch 56/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5828 - acc: 0.7663 - val_loss: 0.5248 - val_acc: 0.7877\n",
      "Epoch 57/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5825 - acc: 0.7694 - val_loss: 0.5224 - val_acc: 0.7877\n",
      "Epoch 58/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5865 - acc: 0.7638 - val_loss: 0.5229 - val_acc: 0.7868\n",
      "Epoch 59/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5876 - acc: 0.7622 - val_loss: 0.5312 - val_acc: 0.7795\n",
      "Epoch 60/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.5827 - acc: 0.7594 - val_loss: 0.5179 - val_acc: 0.7877\n",
      "Epoch 61/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5781 - acc: 0.7730 - val_loss: 0.5161 - val_acc: 0.7943\n",
      "Epoch 62/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5911 - acc: 0.7663 - val_loss: 0.5229 - val_acc: 0.7932\n",
      "Epoch 63/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5836 - acc: 0.7663 - val_loss: 0.5167 - val_acc: 0.7932\n",
      "Epoch 64/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5722 - acc: 0.7733 - val_loss: 0.5234 - val_acc: 0.7831\n",
      "Epoch 65/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5750 - acc: 0.7712 - val_loss: 0.5105 - val_acc: 0.7921\n",
      "Epoch 66/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5781 - acc: 0.7729 - val_loss: 0.5090 - val_acc: 0.7978\n",
      "Epoch 67/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5686 - acc: 0.7708 - val_loss: 0.5083 - val_acc: 0.7974\n",
      "Epoch 68/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5689 - acc: 0.7719 - val_loss: 0.5072 - val_acc: 0.7983\n",
      "Epoch 69/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5757 - acc: 0.7684 - val_loss: 0.5091 - val_acc: 0.7932\n",
      "Epoch 70/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5756 - acc: 0.7680 - val_loss: 0.5209 - val_acc: 0.7901\n",
      "Epoch 71/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5699 - acc: 0.7694 - val_loss: 0.5163 - val_acc: 0.7921\n",
      "Epoch 72/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.5646 - acc: 0.7750 - val_loss: 0.5008 - val_acc: 0.8014\n",
      "Epoch 73/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5697 - acc: 0.7712 - val_loss: 0.5115 - val_acc: 0.7897\n",
      "Epoch 74/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5582 - acc: 0.7752 - val_loss: 0.5027 - val_acc: 0.8016\n",
      "Epoch 75/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.5624 - acc: 0.7783 - val_loss: 0.5017 - val_acc: 0.7981\n",
      "Epoch 76/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5604 - acc: 0.7796 - val_loss: 0.5017 - val_acc: 0.8034\n",
      "Epoch 77/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5613 - acc: 0.7744 - val_loss: 0.5012 - val_acc: 0.7985\n",
      "Epoch 78/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5533 - acc: 0.7800 - val_loss: 0.5002 - val_acc: 0.8003\n",
      "Epoch 79/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5612 - acc: 0.7712 - val_loss: 0.5090 - val_acc: 0.7950\n",
      "Epoch 80/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5571 - acc: 0.7761 - val_loss: 0.5002 - val_acc: 0.7994\n",
      "Epoch 81/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5587 - acc: 0.7783 - val_loss: 0.5016 - val_acc: 0.8014\n",
      "Epoch 82/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5537 - acc: 0.7810 - val_loss: 0.4995 - val_acc: 0.8000\n",
      "Epoch 83/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5602 - acc: 0.7755 - val_loss: 0.4999 - val_acc: 0.8003\n",
      "Epoch 84/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5556 - acc: 0.7773 - val_loss: 0.4949 - val_acc: 0.7983\n",
      "Epoch 85/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5543 - acc: 0.7779 - val_loss: 0.4933 - val_acc: 0.7998\n",
      "Epoch 86/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5566 - acc: 0.7775 - val_loss: 0.5037 - val_acc: 0.7937\n",
      "Epoch 87/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5623 - acc: 0.7763 - val_loss: 0.5027 - val_acc: 0.7989\n",
      "Epoch 88/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5582 - acc: 0.7766 - val_loss: 0.4986 - val_acc: 0.8005\n",
      "Epoch 89/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5576 - acc: 0.7777 - val_loss: 0.4979 - val_acc: 0.8036\n",
      "Epoch 90/196\n",
      "10584/10584 [==============================] - 1s 105us/step - loss: 0.5344 - acc: 0.7823 - val_loss: 0.4941 - val_acc: 0.7987\n",
      "Epoch 91/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5530 - acc: 0.7795 - val_loss: 0.5131 - val_acc: 0.7943\n",
      "Epoch 92/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5436 - acc: 0.7894 - val_loss: 0.4910 - val_acc: 0.8007\n",
      "Epoch 93/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5554 - acc: 0.7789 - val_loss: 0.4905 - val_acc: 0.8056\n",
      "Epoch 94/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5388 - acc: 0.7782 - val_loss: 0.4999 - val_acc: 0.7967\n",
      "Epoch 95/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5432 - acc: 0.7848 - val_loss: 0.4908 - val_acc: 0.8060\n",
      "Epoch 96/196\n",
      "10584/10584 [==============================] - 1s 105us/step - loss: 0.5573 - acc: 0.7793 - val_loss: 0.4919 - val_acc: 0.8053\n",
      "Epoch 97/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5376 - acc: 0.7830 - val_loss: 0.4918 - val_acc: 0.8053\n",
      "Epoch 98/196\n",
      "10584/10584 [==============================] - 1s 105us/step - loss: 0.5428 - acc: 0.7824 - val_loss: 0.4927 - val_acc: 0.8016\n",
      "Epoch 99/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5347 - acc: 0.7847 - val_loss: 0.4878 - val_acc: 0.8045\n",
      "Epoch 100/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5424 - acc: 0.7837 - val_loss: 0.4850 - val_acc: 0.8051\n",
      "Epoch 101/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5358 - acc: 0.7792 - val_loss: 0.4866 - val_acc: 0.8056\n",
      "Epoch 102/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5478 - acc: 0.7871 - val_loss: 0.4938 - val_acc: 0.8042\n",
      "Epoch 103/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5431 - acc: 0.7839 - val_loss: 0.4869 - val_acc: 0.8042\n",
      "Epoch 104/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5491 - acc: 0.7804 - val_loss: 0.4890 - val_acc: 0.8038\n",
      "Epoch 105/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5329 - acc: 0.7879 - val_loss: 0.4853 - val_acc: 0.8029\n",
      "Epoch 106/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5386 - acc: 0.7821 - val_loss: 0.4849 - val_acc: 0.8089\n",
      "Epoch 107/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5388 - acc: 0.7853 - val_loss: 0.4881 - val_acc: 0.8011\n",
      "Epoch 108/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5327 - acc: 0.7858 - val_loss: 0.4921 - val_acc: 0.7989\n",
      "Epoch 109/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5298 - acc: 0.7897 - val_loss: 0.4888 - val_acc: 0.8029\n",
      "Epoch 110/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5423 - acc: 0.7855 - val_loss: 0.4916 - val_acc: 0.8016\n",
      "Epoch 111/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5359 - acc: 0.7886 - val_loss: 0.4878 - val_acc: 0.8005\n",
      "Epoch 112/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5342 - acc: 0.7861 - val_loss: 0.4820 - val_acc: 0.8100\n",
      "Epoch 113/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5326 - acc: 0.7931 - val_loss: 0.4827 - val_acc: 0.8007\n",
      "Epoch 114/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5289 - acc: 0.7881 - val_loss: 0.4857 - val_acc: 0.8056\n",
      "Epoch 115/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5316 - acc: 0.7883 - val_loss: 0.4805 - val_acc: 0.8080\n",
      "Epoch 116/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5368 - acc: 0.7886 - val_loss: 0.4811 - val_acc: 0.8073\n",
      "Epoch 117/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5304 - acc: 0.7881 - val_loss: 0.4777 - val_acc: 0.8056\n",
      "Epoch 118/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5287 - acc: 0.7855 - val_loss: 0.4889 - val_acc: 0.8064\n",
      "Epoch 119/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5275 - acc: 0.7897 - val_loss: 0.4722 - val_acc: 0.8108\n",
      "Epoch 120/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5285 - acc: 0.7875 - val_loss: 0.4788 - val_acc: 0.8102\n",
      "Epoch 121/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5369 - acc: 0.7904 - val_loss: 0.4783 - val_acc: 0.8137\n",
      "Epoch 122/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5278 - acc: 0.7902 - val_loss: 0.4840 - val_acc: 0.8075\n",
      "Epoch 123/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5272 - acc: 0.7901 - val_loss: 0.4789 - val_acc: 0.8111\n",
      "Epoch 124/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5219 - acc: 0.7923 - val_loss: 0.4737 - val_acc: 0.8073\n",
      "Epoch 125/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5288 - acc: 0.7903 - val_loss: 0.4832 - val_acc: 0.8086\n",
      "Epoch 126/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5297 - acc: 0.7887 - val_loss: 0.4725 - val_acc: 0.8102\n",
      "Epoch 127/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5212 - acc: 0.7925 - val_loss: 0.4776 - val_acc: 0.8084\n",
      "Epoch 128/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5231 - acc: 0.7912 - val_loss: 0.4691 - val_acc: 0.8115\n",
      "Epoch 129/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5306 - acc: 0.7881 - val_loss: 0.4688 - val_acc: 0.8159\n",
      "Epoch 130/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.5194 - acc: 0.7964 - val_loss: 0.4724 - val_acc: 0.8113\n",
      "Epoch 131/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5145 - acc: 0.7986 - val_loss: 0.4797 - val_acc: 0.8097\n",
      "Epoch 132/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5208 - acc: 0.7948 - val_loss: 0.4695 - val_acc: 0.8108\n",
      "Epoch 133/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5246 - acc: 0.7946 - val_loss: 0.4690 - val_acc: 0.8128\n",
      "Epoch 134/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5197 - acc: 0.7949 - val_loss: 0.4696 - val_acc: 0.8164\n",
      "Epoch 135/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5174 - acc: 0.7976 - val_loss: 0.4748 - val_acc: 0.8095\n",
      "Epoch 136/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5216 - acc: 0.7953 - val_loss: 0.4768 - val_acc: 0.8157\n",
      "Epoch 137/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5203 - acc: 0.7922 - val_loss: 0.4760 - val_acc: 0.8100\n",
      "Epoch 138/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5187 - acc: 0.7919 - val_loss: 0.4786 - val_acc: 0.8106\n",
      "Epoch 139/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5275 - acc: 0.7916 - val_loss: 0.4709 - val_acc: 0.8117\n",
      "Epoch 140/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5222 - acc: 0.7907 - val_loss: 0.4670 - val_acc: 0.8119\n",
      "Epoch 141/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5244 - acc: 0.7937 - val_loss: 0.4728 - val_acc: 0.8150\n",
      "Epoch 142/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5123 - acc: 0.7956 - val_loss: 0.4708 - val_acc: 0.8106\n",
      "Epoch 143/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5161 - acc: 0.7899 - val_loss: 0.4694 - val_acc: 0.8100\n",
      "Epoch 144/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5183 - acc: 0.7950 - val_loss: 0.4691 - val_acc: 0.8075\n",
      "Epoch 145/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5186 - acc: 0.7979 - val_loss: 0.4702 - val_acc: 0.8124\n",
      "Epoch 146/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5153 - acc: 0.7942 - val_loss: 0.4793 - val_acc: 0.8060\n",
      "Epoch 147/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5093 - acc: 0.7960 - val_loss: 0.4860 - val_acc: 0.8089\n",
      "Epoch 148/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5238 - acc: 0.7974 - val_loss: 0.4675 - val_acc: 0.8137\n",
      "Epoch 149/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5182 - acc: 0.7945 - val_loss: 0.4652 - val_acc: 0.8133\n",
      "Epoch 150/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5102 - acc: 0.7987 - val_loss: 0.4594 - val_acc: 0.8188\n",
      "Epoch 151/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5106 - acc: 0.7978 - val_loss: 0.4585 - val_acc: 0.8150\n",
      "Epoch 152/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4990 - acc: 0.8006 - val_loss: 0.4661 - val_acc: 0.8131\n",
      "Epoch 153/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5111 - acc: 0.7997 - val_loss: 0.4624 - val_acc: 0.8181\n",
      "Epoch 154/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5139 - acc: 0.7984 - val_loss: 0.4669 - val_acc: 0.8139\n",
      "Epoch 155/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5157 - acc: 0.7976 - val_loss: 0.4744 - val_acc: 0.8106\n",
      "Epoch 156/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5152 - acc: 0.7993 - val_loss: 0.4654 - val_acc: 0.8139\n",
      "Epoch 157/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5171 - acc: 0.7931 - val_loss: 0.4649 - val_acc: 0.8155\n",
      "Epoch 158/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5094 - acc: 0.8029 - val_loss: 0.4656 - val_acc: 0.8166\n",
      "Epoch 159/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5058 - acc: 0.7971 - val_loss: 0.4641 - val_acc: 0.8124\n",
      "Epoch 160/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5087 - acc: 0.7988 - val_loss: 0.4626 - val_acc: 0.8179\n",
      "Epoch 161/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5122 - acc: 0.7975 - val_loss: 0.4733 - val_acc: 0.8166\n",
      "Epoch 162/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5158 - acc: 0.7952 - val_loss: 0.4625 - val_acc: 0.8190\n",
      "Epoch 163/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5116 - acc: 0.8002 - val_loss: 0.4600 - val_acc: 0.8164\n",
      "Epoch 164/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5098 - acc: 0.8009 - val_loss: 0.4611 - val_acc: 0.8168\n",
      "Epoch 165/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5022 - acc: 0.8042 - val_loss: 0.4652 - val_acc: 0.8155\n",
      "Epoch 166/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5043 - acc: 0.8031 - val_loss: 0.4615 - val_acc: 0.8148\n",
      "Epoch 167/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5068 - acc: 0.8005 - val_loss: 0.4615 - val_acc: 0.8157\n",
      "Epoch 168/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5143 - acc: 0.7988 - val_loss: 0.4636 - val_acc: 0.8164\n",
      "Epoch 169/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5172 - acc: 0.7962 - val_loss: 0.4564 - val_acc: 0.8183\n",
      "Epoch 170/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5071 - acc: 0.7944 - val_loss: 0.4674 - val_acc: 0.8126\n",
      "Epoch 171/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4981 - acc: 0.7999 - val_loss: 0.4573 - val_acc: 0.8183\n",
      "Epoch 172/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5101 - acc: 0.7969 - val_loss: 0.4636 - val_acc: 0.8150\n",
      "Epoch 173/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5005 - acc: 0.7961 - val_loss: 0.4623 - val_acc: 0.8208\n",
      "Epoch 174/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.4990 - acc: 0.8033 - val_loss: 0.4601 - val_acc: 0.8170\n",
      "Epoch 175/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5058 - acc: 0.7997 - val_loss: 0.4620 - val_acc: 0.8157\n",
      "Epoch 176/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5006 - acc: 0.7998 - val_loss: 0.4608 - val_acc: 0.8142\n",
      "Epoch 177/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4992 - acc: 0.8082 - val_loss: 0.4592 - val_acc: 0.8170\n",
      "Epoch 178/196\n",
      "10584/10584 [==============================] - 1s 104us/step - loss: 0.5118 - acc: 0.8009 - val_loss: 0.4593 - val_acc: 0.8172\n",
      "Epoch 179/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.5134 - acc: 0.8006 - val_loss: 0.4614 - val_acc: 0.8186\n",
      "Epoch 180/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.4967 - acc: 0.8007 - val_loss: 0.4694 - val_acc: 0.8155\n",
      "Epoch 181/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4902 - acc: 0.8023 - val_loss: 0.4648 - val_acc: 0.8161\n",
      "Epoch 182/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5057 - acc: 0.7969 - val_loss: 0.4574 - val_acc: 0.8194\n",
      "Epoch 183/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5044 - acc: 0.8047 - val_loss: 0.4642 - val_acc: 0.8179\n",
      "Epoch 184/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.4946 - acc: 0.8034 - val_loss: 0.4581 - val_acc: 0.8212\n",
      "Epoch 185/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5023 - acc: 0.8042 - val_loss: 0.4620 - val_acc: 0.8128\n",
      "Epoch 186/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5033 - acc: 0.7995 - val_loss: 0.4568 - val_acc: 0.8201\n",
      "Epoch 187/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.4980 - acc: 0.8043 - val_loss: 0.4574 - val_acc: 0.8190\n",
      "Epoch 188/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.4906 - acc: 0.8008 - val_loss: 0.4594 - val_acc: 0.8208\n",
      "Epoch 189/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.5053 - acc: 0.7996 - val_loss: 0.4596 - val_acc: 0.8197\n",
      "Epoch 190/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4973 - acc: 0.8065 - val_loss: 0.4595 - val_acc: 0.8234\n",
      "Epoch 191/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4990 - acc: 0.8041 - val_loss: 0.4602 - val_acc: 0.8155\n",
      "Epoch 192/196\n",
      "10584/10584 [==============================] - 1s 102us/step - loss: 0.5022 - acc: 0.8042 - val_loss: 0.4625 - val_acc: 0.8159\n",
      "Epoch 193/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.5107 - acc: 0.8029 - val_loss: 0.4579 - val_acc: 0.8175\n",
      "Epoch 194/196\n",
      "10584/10584 [==============================] - 1s 103us/step - loss: 0.4955 - acc: 0.8068 - val_loss: 0.4602 - val_acc: 0.8159\n",
      "Epoch 195/196\n",
      "10584/10584 [==============================] - 1s 101us/step - loss: 0.4981 - acc: 0.8071 - val_loss: 0.4562 - val_acc: 0.8179\n",
      "Epoch 196/196\n",
      "10584/10584 [==============================] - 1s 100us/step - loss: 0.4950 - acc: 0.8043 - val_loss: 0.4565 - val_acc: 0.8234\n",
      "4536/4536 [==============================] - 0s 67us/step\n"
     ]
    }
   ],
   "source": [
    "model = kr.Sequential([\n",
    "    kr.layers.Dense(112, input_shape = (54, ), activation = tf.nn.relu),\n",
    "    kr.layers.Dropout(0.4),\n",
    "    kr.layers.Dense(56, activation = tf.nn.relu),\n",
    "    kr.layers.Dropout(0.4),\n",
    "    kr.layers.Dense(28, activation = tf.nn.relu),\n",
    "    kr.layers.Dropout(0.4),\n",
    "    kr.layers.Dense(number_of_classes, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(), \n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history4 = model.fit(x_train, y_train, epochs = 196, batch_size = 54, validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L02dtml-Munj"
   },
   "source": [
    "### Visualize the training\n",
    "- use matplotlib.pyplot to visualize the keras history\n",
    "- plot both the training accuracy and the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "KxZSUEOxMunk",
    "outputId": "91b579e1-41d7-4c5a-f1cc-3abd6b565a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4536/4536 [==============================] - 0s 68us/step\n",
      "Test accuracy: 0.8234126985178213\n",
      "Test loss: 0.45651696619743604\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGOb/ADGGBJZVT4ToAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f097268ba58>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "plot_hist(history4)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VDL_Exercise_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
